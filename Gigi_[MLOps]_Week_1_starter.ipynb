{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Gigi [MLOps] Week 1 starter.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gigikenneth/mlops-wk1/blob/main/Gigi_%5BMLOps%5D_Week_1_starter.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "\n",
        "### Problem\n",
        "\n",
        "In the project this week, we will build a machine learning text classifier to predict news categories from the news article text. \n",
        "1. We will iterate on classification models with increasing level of complexity and improved performance. \n",
        "2. We will look at the impact of training data size on model performance.\n",
        "3. [advanced] As an extension, you will explore how to augment data efficiently to your existing training data (efficiency measured as improvement in performance normalized by volume of data augmented). The reason to consider efficiency is that in most real-world scenarios, additional data is typically unlabeled. In order to augment it to your training data, you have to get it annotated which incurs some time & money (either yoursself or through one of the existing annotation tools such as Labelbox, Scale etc). \n",
        "\n",
        "Throughout the project there are suggested model architectures that we expect to work reasonably well for this problem. But if you wish to extend/modify any part of this pipeline, or explore new model architectures you should definitely feel free to do so.\n",
        "\n",
        "### Deliverables\n",
        "1. Train average word vector classifier and report model performance for training size = [500, 1000, 2000, 5000, 10000, 25000]\n",
        "2. Train transformer encoder classifier and report model performance for training size = [500, 1000, 2000, 5000, 10000, 25000]\n",
        "3. Report performance improvement on the test dataset from naive dataset augmentation outlined in the this notebook\n",
        "4. [stretch] Experiment with advanced data augmentation techniques (a few ideas & pointers given in the notebook below)\n"
      ],
      "metadata": {
        "id": "vCJ3pvJKY_xV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Bj6YqjmQlICF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step1: Prereqs & Installation\n",
        "\n",
        "Download & Import all the necessary libraries we need throughout the project."
      ],
      "metadata": {
        "id": "4FNP8FSfZIed"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install all the required dependencies for the project\n",
        "\n",
        "!pip install numpy\n",
        "!pip install scikit-learn\n",
        "!pip install gensim\n",
        "!pip install sentence-transformers\n",
        "!pip install matplotlib"
      ],
      "metadata": {
        "id": "l1LsWxD0ZF3b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "627d7ef4-b029-4894-b649-4a17f02f58b3"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (1.21.6)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (1.0.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (3.1.0)\n",
            "Requirement already satisfied: numpy>=1.14.6 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.21.6)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.7.3)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.1.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (3.6.0)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.21.6)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.15.0)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.7.3)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (5.2.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.7/dist-packages (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.0.2)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.13.0+cu113)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (4.20.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (4.64.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.7.3)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.1.96)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.12.0+cu113)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.21.6)\n",
            "Requirement already satisfied: huggingface-hub>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.8.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (3.7)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (21.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.12.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2.23.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.1.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.7.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (6.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.9->huggingface-hub>=0.4.0->sentence-transformers) (3.0.9)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.12.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2022.6.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->huggingface-hub>=0.4.0->sentence-transformers) (3.8.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk->sentence-transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk->sentence-transformers) (1.1.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2022.6.15)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.0.4)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sentence-transformers) (3.1.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->sentence-transformers) (7.1.2)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (3.2.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.4.3)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.21.6)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (0.11.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib) (4.1.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Package imports that will be needed for this project\n",
        "\n",
        "import numpy as np\n",
        "import json\n",
        "from collections import Counter\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from gensim.utils import tokenize as gensim_tokenizer\n",
        "import gensim.downloader as gensim_downloader\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from pprint import pprint\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# [TO BE IMPLEMENTED] \n",
        "# Add any other imports needed below depending on the model architectures you are using. For e.g.\n",
        "# from sklearn.linear_model import LogisticRegression"
      ],
      "metadata": {
        "id": "yiDpaCRTZOKL"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Global Constants\n",
        "LABEL_SET = [\n",
        "    'Business', \n",
        "    'Sci/Tech',\n",
        "    'Software and Developement',\n",
        "    'Entertainment',\n",
        "    'Sports',\n",
        "    'Health',\n",
        "    'Toons',\n",
        "    'Music Feeds'\n",
        "]\n",
        "\n",
        "WORD_VECTOR_MODEL = 'glove-wiki-gigaword-100'\n",
        "SENTENCE_TRANSFORMER_MODEL = 'all-mpnet-base-v2'\n",
        "\n",
        "TRAIN_SIZE_EVALS = [500, 1000, 2000, 5000, 10000, 25000]\n",
        "#TRAIN_SIZE_EVALS = [500, 1000]\n",
        "EPS = 0.001\n",
        "SEED = 0\n",
        "\n",
        "np.random.seed(SEED)"
      ],
      "metadata": {
        "id": "p9asDVPMZlf3"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Download & Load Datasets \n",
        "\n",
        "[AG News](http://groups.di.unipi.it/~gulli/AG_corpus_of_news_articles.html) is a collection of more than 1 million news articles gathered from more than 2000 news sources by an academic news search engine. The news topic classification dataset & benchmark was first used in [Character-level Convolutional Networks for Text Classification (NIPS 2015)](https://arxiv.org/abs/1509.01626). The dataset has the text description (summary) of the news article along with some metadata. **For this project, we will use a slightly modified (cleaned up) version of this dataset** \n",
        "\n",
        "Schema:\n",
        "* Source - News publication source\n",
        "* URL - URL of the news article\n",
        "* Title - Title of the news article\n",
        "* Description - Summary description of the news article\n",
        "* Category (Label) - News category\n",
        "\n",
        "Sample row in this dataset:\n",
        "```\n",
        "{\n",
        "    'description': 'A capsule carrying solar material from the Genesis space '\n",
        "                'probe has made a crash landing at a US Air Force training '\n",
        "                'facility in the US state of Utah.',\n",
        "    'id': 86273,\n",
        "    'label': 'Entertainment',\n",
        "    'source': 'Voice of America',\n",
        "    'title': 'Capsule from Genesis Space Probe Crashes in Utah Desert',\n",
        "    'url': 'http://www.sciencedaily.com/releases/2004/09/040908090621.htm'\n",
        " }\n",
        "```\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "gLu2IBiqZsgs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from urllib.request import urlopen\n",
        "from io import BytesIO\n",
        "from zipfile import ZipFile\n",
        "\n",
        "DIRECTORY_NAME = \"data\"\n",
        "DOWNLOAD_URL = 'https://corise-mlops.s3.us-west-2.amazonaws.com/project1/agnews.zip'\n",
        "\n",
        "def download_dataset():\n",
        "    \"\"\"\n",
        "    Download the dataset. The zip contains three files: train.json, test.json and unlabeled.json \n",
        "    \"\"\"\n",
        "    http_response = urlopen(DOWNLOAD_URL)\n",
        "    zipfile = ZipFile(BytesIO(http_response.read()))\n",
        "    zipfile.extractall(path=DIRECTORY_NAME)\n",
        "\n",
        "# Expensive operation so we should just do this once\n",
        "download_dataset()"
      ],
      "metadata": {
        "id": "jFGaBYdSZtqM"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Datasets = {}\n",
        "\n",
        "for ds in ['train', 'test', 'augment']:\n",
        "    with open('data/{}.json'.format(ds), 'r') as f:\n",
        "        Datasets[ds] = json.load(f)\n",
        "    print(\"Loaded Dataset {0} with {1} rows\".format(ds, len(Datasets[ds])))\n",
        "\n",
        "print(\"\\nExample train row:\\n\")\n",
        "pprint(Datasets['train'][0])\n",
        "\n",
        "print(\"\\nExample test row:\\n\")\n",
        "pprint(Datasets['test'][0])"
      ],
      "metadata": {
        "id": "zYnT4BIcZ5vX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84b2d32f-3283-4f73-b8e1-ecf54ea19fd0"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded Dataset train with 25000 rows\n",
            "Loaded Dataset test with 50000 rows\n",
            "Loaded Dataset augment with 150000 rows\n",
            "\n",
            "Example train row:\n",
            "\n",
            "{'description': 'A capsule carrying solar material from the Genesis space '\n",
            "                'probe has made a crash landing at a US Air Force training '\n",
            "                'facility in the US state of Utah.',\n",
            " 'id': 86273,\n",
            " 'label': 'Entertainment',\n",
            " 'source': 'Voice of America',\n",
            " 'title': 'Capsule from Genesis Space Probe Crashes in Utah Desert',\n",
            " 'url': 'http://www.sciencedaily.com/releases/2004/09/040908090621.htm'}\n",
            "\n",
            "Example test row:\n",
            "\n",
            "{'description': 'AP - Ellis L. Marsalis Sr., the patriarch of a family of '\n",
            "                'world famous jazz musicians, including grandson Wynton '\n",
            "                'Marsalis, has died. He was 96.',\n",
            " 'id': 143852,\n",
            " 'label': 'Entertainment',\n",
            " 'source': 'Yahoo Entertainment',\n",
            " 'title': 'Music Patriarch Marsalis Sr. Dies (AP)',\n",
            " 'url': 'http://us.rd.yahoo.com/dailynews/rss/entertainment/*http://story.news.yahoo.com/news?tmpl=story2 '\n",
            "        'u=/ap/20040922/ap_en_mu/obit_marsalis'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, Y_train = [], []\n",
        "X_test, Y_true = [], []\n",
        "X_augment, Y_augment = [], []\n",
        "\n",
        "for row in Datasets['train']:\n",
        "    X_train.append(row['description'])\n",
        "    Y_train.append(row['label'])\n",
        "\n",
        "for row in Datasets['test']:\n",
        "    X_test.append(row['description'])\n",
        "    Y_true.append(row['label'])\n",
        "\n",
        "for row in Datasets['augment']:\n",
        "    X_augment.append(row['description'])\n",
        "    Y_augment.append(row['label'])"
      ],
      "metadata": {
        "id": "TcwebhuYZ8Kb"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: [Modeling part 1] Word vectors"
      ],
      "metadata": {
        "id": "KoayaMr8aBwp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the word vector model\n",
        "word_vector_model = gensim_downloader.load(WORD_VECTOR_MODEL)\n",
        "\n",
        "# Sanity check\n",
        "print(word_vector_model.most_similar(\"cat\"))\n",
        "print(word_vector_model['cat'])"
      ],
      "metadata": {
        "id": "XPhazZRNaC3E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af32f5c9-85d1-4ab1-ccaf-d399b3db3390"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('dog', 0.8798074722290039), ('rabbit', 0.7424426674842834), ('cats', 0.7323004007339478), ('monkey', 0.7288709878921509), ('pet', 0.7190139889717102), ('dogs', 0.7163872718811035), ('mouse', 0.6915250420570374), ('puppy', 0.6800068020820618), ('rat', 0.6641027331352234), ('spider', 0.6501135230064392)]\n",
            "[ 0.23088    0.28283    0.6318    -0.59411   -0.58599    0.63255\n",
            "  0.24402   -0.14108    0.060815  -0.7898    -0.29102    0.14287\n",
            "  0.72274    0.20428    0.1407     0.98757    0.52533    0.097456\n",
            "  0.8822     0.51221    0.40204    0.21169   -0.013109  -0.71616\n",
            "  0.55387    1.1452    -0.88044   -0.50216   -0.22814    0.023885\n",
            "  0.1072     0.083739   0.55015    0.58479    0.75816    0.45706\n",
            " -0.28001    0.25225    0.68965   -0.60972    0.19578    0.044209\n",
            " -0.31136   -0.68826   -0.22721    0.46185   -0.77162    0.10208\n",
            "  0.55636    0.067417  -0.57207    0.23735    0.4717     0.82765\n",
            " -0.29263   -1.3422    -0.099277   0.28139    0.41604    0.10583\n",
            "  0.62203    0.89496   -0.23446    0.51349    0.99379    1.1846\n",
            " -0.16364    0.20653    0.73854    0.24059   -0.96473    0.13481\n",
            " -0.0072484  0.33016   -0.12365    0.27191   -0.40951    0.021909\n",
            " -0.6069     0.40755    0.19566   -0.41802    0.18636   -0.032652\n",
            " -0.78571   -0.13847    0.044007  -0.084423   0.04911    0.24104\n",
            "  0.45273   -0.18682    0.46182    0.089068  -0.18185   -0.01523\n",
            " -0.7368    -0.14532    0.15104   -0.71493  ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class WordVectorFeaturizer(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, dim, word_vector_model):\n",
        "        self.dim = dim\n",
        "        self.word_vector_model = word_vector_model\n",
        "        # you can add any other params to be passed to the constructor here\n",
        "    \n",
        "    #estimator. Since we don't have to learn anything in the featurizer, this is a no-op\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "    \n",
        "    #transformation: return the average word vector of each token in the document\n",
        "    def transform(self, X, y=None):\n",
        "        \"\"\"\n",
        "        [TO BE IMPLEMENTED]\n",
        "        \n",
        "        Goal: WordVectorFeaturizer's transform() method converts the raw text document\n",
        "        into a feature vector to be passed as input to the classifier.\n",
        "            \n",
        "        Given below is a dummy implementation that always maps it to a zero vector.\n",
        "        You have to implement this function so it does two things:\n",
        "            (1) Convert the raw text document into a list of tokens\n",
        "            (2) Map each token to a word vector (using self.word_vector_model)\n",
        "            (3) Return the mean vector of the sequence of word vectors\n",
        "            *(4) Create vectors for words outside the vocabulary so it doesn't return an error\n",
        "        This will be our feature representation of the document\n",
        "        \"\"\"\n",
        "\n",
        "        X_t = []\n",
        "        for doc in X:\n",
        "          array = []\n",
        "          #split the strings into individual words\n",
        "          for element in doc.split():\n",
        "            try:\n",
        "              array.append(word_vector_model([element]))\n",
        "            except:\n",
        "              continue\n",
        "          if len(array) != 0:\n",
        "            X_t.append(np.mean(array, axis=0))\n",
        "          else:\n",
        "            X_t.append(np.zeros(self.dim))\n",
        "            # TODO: replace this dummy implementation\n",
        "            #X_t.append(np.zeros(self.dim))\n",
        "        return X_t"
      ],
      "metadata": {
        "id": "-HRUBp_BaFZ9"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "models = {}\n",
        "\n",
        "for n in TRAIN_SIZE_EVALS:\n",
        "    print(\"Evaluating for training data size = {}\".format(n))\n",
        "    X_train_i = X_train[:n]\n",
        "    Y_train_i = Y_train[:n]\n",
        "\n",
        "    \"\"\"\n",
        "    [TO BE IMPLEMENTED]\n",
        "        \n",
        "    Goal: initialized below is a dummy sklearn Pipeline object with no steps.\n",
        "    You have to replace it with a pipeline object which contains at least two steps:\n",
        "    (1) mapping the input document to a feature vector (using WordVectorFeaturizer)\n",
        "    (2) a classifier that predicts the class label using the feature output of first step\n",
        "\n",
        "    You can add other steps to preproces, post-process your data as you see fit. \n",
        "    You can also try any sklearn model architecture you want, but a linear classifier\n",
        "    will do just fine to start with\n",
        "\n",
        "    e.g. \n",
        "    pipeline = Pipeline([\n",
        "        ('featurizer', <your WordVectorFeaturizer class instance here>),\n",
        "        ('classifier', <your sklearn classifier class instance here>)\n",
        "    ])\n",
        "    \"\"\"\n",
        "    # pipeline =  Pipeline([\n",
        "    #     'featurizer', WordVectorFeaturizer(dim=100,\n",
        "    #                                        word_vector_model)\n",
        "    \n",
        "    #     'classifier', LogisticRegression\n",
        "    # ])\n",
        "\n",
        "    pipeline = Pipeline([('featurizer', WordVectorFeaturizer(dim=100, word_vector_model = word_vector_model)),\n",
        "        ('classifier', LogisticRegression(solver='saga', tol=.001))])\n",
        "\n",
        "    \n",
        "    # train\n",
        "    pipeline.fit(X_train_i, Y_train_i)\n",
        "    # predict\n",
        "    Y_pred_i = pipeline.predict(X_test)\n",
        "    # record results\n",
        "    models[n] = {\n",
        "        'pipeline': pipeline,\n",
        "        'test_predictions': Y_pred_i,\n",
        "        'accuracy': accuracy_score(Y_true, Y_pred_i),\n",
        "        'f1': f1_score(Y_true, Y_pred_i, average='weighted'),\n",
        "        'errors': sum([x != y for (x, y) in zip(Y_true, Y_pred_i)])\n",
        "    }\n",
        "    print(\"Accuracy on test set: {}\".format(accuracy_score(Y_true, Y_pred_i)))"
      ],
      "metadata": {
        "id": "n2zIahH6aHJ9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0905e149-c440-4af9-dd67-abdb817f8dbe"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating for training data size = 500\n",
            "Accuracy on test set: 0.2431\n",
            "Evaluating for training data size = 1000\n",
            "Accuracy on test set: 0.27502\n",
            "Evaluating for training data size = 2000\n",
            "Accuracy on test set: 0.27502\n",
            "Evaluating for training data size = 5000\n",
            "Accuracy on test set: 0.2431\n",
            "Evaluating for training data size = 10000\n",
            "Accuracy on test set: 0.2431\n",
            "Evaluating for training data size = 25000\n",
            "Accuracy on test set: 0.27502\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4: [Modeling part 2] Pretrained Transformers"
      ],
      "metadata": {
        "id": "OBxVNbWBaMhc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the pretrained transformer model\n",
        "sentence_transformer_model = SentenceTransformer(\n",
        "    'sentence-transformers/{model}'.format(model=SENTENCE_TRANSFORMER_MODEL))\n",
        "\n",
        "# Sanity check\n",
        "example_encoding = sentence_transformer_model.encode(\n",
        "    \"This is an example sentence\",\n",
        "    normalize_embeddings=True\n",
        ")\n",
        "\n",
        "print(example_encoding)\n"
      ],
      "metadata": {
        "id": "27TJGTZfavys",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10b2a865-4abc-4cb3-8091-04436064447d"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 2.25026179e-02 -7.82917142e-02 -2.30307244e-02 -5.10009658e-03\n",
            " -8.03404525e-02  3.91321592e-02  1.13428263e-02  3.46482964e-03\n",
            " -2.94574499e-02 -1.88930500e-02  9.47433487e-02  2.92747449e-02\n",
            "  3.94859836e-02 -4.63165939e-02  2.54246984e-02 -3.21999528e-02\n",
            "  6.21928386e-02  1.55591518e-02 -4.67794836e-02  5.03901616e-02\n",
            "  1.46113932e-02  2.31413580e-02  1.22067202e-02  2.50696465e-02\n",
            "  2.93655926e-03 -4.19821963e-02 -4.01039328e-03 -2.27843784e-02\n",
            " -7.68592861e-03 -3.31090540e-02  3.22118662e-02 -2.09992100e-02\n",
            "  1.16730388e-02 -9.85074043e-02  1.77932634e-06 -2.29931921e-02\n",
            " -1.31140687e-02 -2.80222706e-02 -6.99970126e-02  2.59314254e-02\n",
            " -2.89502069e-02  8.76335949e-02 -1.20919021e-02  3.98605317e-02\n",
            " -3.31381895e-02  3.59108038e-02  3.46099287e-02  6.49784133e-02\n",
            " -3.00817527e-02  6.98188320e-02 -3.99512751e-03 -1.01597141e-03\n",
            " -3.50185595e-02 -4.36567590e-02  5.08026108e-02  4.68757935e-02\n",
            "  5.39663583e-02 -4.03008610e-02  3.20132799e-03  1.36617851e-02\n",
            "  3.82188521e-02 -3.23847006e-03 -7.84604752e-04 -1.71188600e-02\n",
            "  6.90438692e-03 -1.09237498e-02  8.63303337e-03 -1.82357933e-02\n",
            "  1.87930707e-02  1.54990284e-02  1.02149611e-02 -2.48379493e-03\n",
            "  1.03153447e-02  6.24887310e-02  3.60321160e-03 -6.26626797e-03\n",
            " -2.03406196e-02 -6.72352966e-03 -3.54771577e-02  3.43538448e-02\n",
            "  6.72282055e-02  9.06872824e-02  1.32441027e-02  2.06592437e-02\n",
            " -2.78685484e-02  4.29694653e-02 -4.66859229e-02  1.50115592e-02\n",
            " -6.62284195e-02 -2.27593798e-02 -6.24990165e-02 -2.58455537e-02\n",
            "  7.31316919e-04  1.14652757e-02  5.66384122e-02  2.06242804e-03\n",
            " -4.09249030e-02 -4.55051363e-02  1.66958198e-02 -8.31556693e-02\n",
            "  2.09071557e-03 -8.70920438e-03  1.07640619e-04  3.37445438e-02\n",
            "  5.60342427e-03 -1.66981462e-02  4.47909385e-02  6.31804392e-03\n",
            " -6.45904019e-02  5.29103540e-02  1.93019398e-02 -6.20157924e-03\n",
            " -1.18759960e-01  3.55964079e-02 -2.28864737e-02 -1.51872477e-02\n",
            " -5.92660159e-03 -1.57177186e-04  1.07069639e-02  3.86090344e-03\n",
            " -6.87014759e-02 -1.69752669e-02 -2.79730018e-02  2.80480720e-02\n",
            "  2.47794129e-02  1.20279370e-02 -6.86393455e-02  4.92765121e-02\n",
            "  1.87576972e-02 -2.42343582e-02 -2.05291733e-02 -1.07933655e-02\n",
            "  2.46493146e-02 -3.33322659e-02 -3.28397937e-02  2.91978400e-02\n",
            "  4.92033176e-02 -7.13362359e-03 -1.63389742e-02  1.78586738e-03\n",
            "  2.18068305e-02 -8.90230834e-02 -3.37051526e-02  5.77224325e-03\n",
            " -4.56566662e-02  3.39890979e-02  3.52784097e-02 -3.12628038e-02\n",
            "  8.10835790e-03  2.68614627e-02 -2.23900704e-03  2.81266831e-02\n",
            " -1.75384041e-02 -1.44589907e-02 -3.33481170e-02 -1.62954740e-02\n",
            "  9.70038921e-02 -8.11064895e-03 -2.46668793e-02 -5.87456562e-02\n",
            "  8.74912483e-04  1.67235360e-02  9.15387738e-03 -1.17979187e-03\n",
            " -2.93022627e-03  4.22465103e-03 -2.16529202e-02  4.29305658e-02\n",
            " -5.86095601e-02  3.13417688e-02 -1.29514211e-03 -1.11298151e-02\n",
            " -2.82019693e-02  8.77324268e-02  2.06880625e-02  1.41398842e-02\n",
            "  1.38229271e-02 -1.94184165e-02 -9.01034847e-02 -3.81479715e-03\n",
            " -2.91157165e-03  3.09753530e-02 -1.18769342e-02  1.88288968e-02\n",
            " -4.59066629e-02  4.98210378e-02 -8.39175750e-03 -4.29713614e-02\n",
            " -3.23598944e-02 -3.83801237e-02 -2.99748424e-02  3.69881578e-02\n",
            " -4.44592582e-03 -1.94783956e-02 -2.71528233e-02  2.43246574e-02\n",
            "  9.16411169e-04  5.85004836e-02  1.92713961e-02 -2.57291384e-02\n",
            "  4.08677422e-02  4.36861068e-03  5.13519384e-02  1.57080889e-02\n",
            " -2.46329308e-02 -9.79608856e-03  2.06121709e-03 -4.66644503e-02\n",
            "  3.19584645e-02 -3.73425446e-02  9.35152993e-02  1.85420848e-02\n",
            " -2.60215327e-02  8.05766508e-03 -6.37755002e-05 -4.74147918e-03\n",
            "  2.17362624e-02 -4.03623544e-02 -3.97235043e-02  6.60505146e-02\n",
            " -3.20185348e-02 -1.52356559e-02 -1.53095312e-02  5.58148790e-03\n",
            "  3.96784917e-02 -5.98881058e-02 -2.94910427e-02 -1.53479502e-02\n",
            " -3.32981311e-02 -1.35856066e-02 -2.23695189e-02  1.81130297e-03\n",
            " -2.53528531e-04  7.30922353e-03 -4.96328883e-02  3.74633968e-02\n",
            " -4.42486890e-02 -8.77882093e-02 -1.95525270e-02 -7.44620413e-02\n",
            " -5.28370449e-03 -8.59955512e-03  1.65657774e-02  1.99179053e-02\n",
            " -9.94187966e-03 -2.85208947e-03  7.21454248e-02 -1.99029408e-02\n",
            "  2.95140445e-02 -5.97200990e-02  5.00880741e-02 -2.54911445e-02\n",
            "  2.33916156e-02 -7.12675974e-03  7.38676172e-03 -7.17939734e-02\n",
            "  9.14958597e-04  2.19873544e-02  4.15912969e-03  1.79543924e-02\n",
            "  6.32213801e-02 -2.47943797e-03 -5.26576489e-03  2.34970953e-02\n",
            " -2.61955317e-02 -3.71229909e-02  2.15678588e-02 -5.85354939e-02\n",
            " -1.79577358e-02 -1.20004797e-02  8.96561716e-04 -1.47689134e-02\n",
            "  4.96944897e-02  6.97952276e-03  2.64367703e-02  4.61773947e-02\n",
            "  3.20434086e-02 -3.66006121e-02 -5.08424686e-03  6.88665658e-02\n",
            "  5.68004362e-02 -1.46778375e-02 -4.78474833e-02  1.21871810e-02\n",
            " -2.50420831e-02  3.12442686e-02 -1.79439969e-02 -3.05826347e-02\n",
            "  1.71709212e-03  7.02126846e-02  5.67383580e-02 -1.79368258e-02\n",
            "  2.44000815e-02 -2.86526401e-02 -1.15867611e-02 -2.70408560e-02\n",
            "  3.95130925e-02  4.29957472e-02  2.90972833e-02  2.80838721e-02\n",
            " -4.62748893e-02 -4.28297929e-03  1.19901933e-02 -1.20225139e-02\n",
            " -9.46942717e-03  2.35066339e-02 -3.00627910e-02 -1.69607997e-02\n",
            " -1.59736804e-03 -1.30609516e-02  5.35884872e-02  2.53782701e-02\n",
            "  2.60250326e-02  6.27413392e-02 -2.26463098e-02  6.58664387e-03\n",
            " -3.48779112e-02 -8.88993032e-03 -3.32267061e-02 -1.81599967e-02\n",
            " -6.45447895e-03  1.02021694e-02 -1.25164194e-02  4.20163982e-02\n",
            "  1.12152370e-02 -2.13345326e-02  1.05621303e-02  1.99820623e-02\n",
            "  1.83803979e-02  3.29691009e-03 -8.70438572e-03  1.90763418e-02\n",
            " -4.41014469e-02  9.57714915e-02  2.73615625e-02  1.76533647e-02\n",
            " -2.20417958e-02  3.70631516e-02 -6.52653573e-04 -1.44511722e-02\n",
            "  1.09790852e-02 -8.40485282e-03 -3.26200901e-03 -2.20720563e-02\n",
            " -1.90347489e-02 -1.60558112e-02 -4.08147834e-02  1.11608747e-02\n",
            " -6.02423027e-02 -6.96681216e-02 -1.73304304e-02  2.87935082e-02\n",
            " -6.79623410e-02 -3.13759260e-02 -5.51356785e-02 -2.03582197e-02\n",
            "  2.89013032e-02  1.37794074e-02  6.80501433e-03 -2.43215146e-03\n",
            "  7.21530989e-02 -1.17460068e-03 -3.57215516e-02  3.54787074e-02\n",
            " -1.96369714e-03 -7.76635297e-03  3.01939417e-02  1.85422190e-02\n",
            " -5.39993979e-02  3.32430005e-02  5.73031325e-03  1.33993253e-02\n",
            "  4.51609679e-03  4.88920249e-02 -3.14347036e-02  3.62168923e-02\n",
            "  3.65449190e-02 -4.79209945e-02 -1.44875636e-02  4.93125916e-02\n",
            "  2.86978651e-02 -5.51462770e-02  2.74743252e-02  1.27805164e-02\n",
            " -7.04631656e-02  7.69065972e-03 -5.24684135e-03 -5.33922315e-02\n",
            " -1.70808993e-02  4.77676541e-02  2.38064844e-02 -4.09796871e-02\n",
            " -1.27406381e-02  4.66342606e-02  5.03483927e-03  6.60549989e-03\n",
            "  2.90571991e-02  4.15974073e-02 -3.82126644e-02 -1.14388196e-02\n",
            "  1.71640627e-02  5.70877083e-03  1.07285539e-02 -1.80592779e-02\n",
            " -5.06379306e-02  4.54925112e-02  1.40738105e-02  4.25583348e-02\n",
            " -3.22351083e-02  4.17672582e-02  1.14987073e-02  3.92394327e-03\n",
            "  2.04459336e-02  1.52545683e-02  3.80402617e-02  2.54580863e-02\n",
            " -4.69273143e-03  1.83214266e-02  2.76016369e-02 -2.89157573e-02\n",
            " -4.98981029e-02 -1.61939561e-02  9.87022668e-02 -4.26361673e-02\n",
            " -1.88478492e-02 -1.07012335e-02 -3.21414880e-02  4.15321402e-02\n",
            " -2.38698963e-02  8.39932077e-03 -1.00903132e-03 -3.11340541e-02\n",
            " -3.86489704e-02 -3.06742489e-02 -3.88901606e-02 -3.65617387e-02\n",
            "  3.29428096e-03  2.00938433e-02  2.30732355e-02 -4.77465838e-02\n",
            "  8.55971593e-03  2.21940633e-02  1.49231061e-01 -1.91771593e-02\n",
            "  1.43476827e-02  4.39948849e-02 -2.27765762e-03  1.38103811e-03\n",
            "  3.23159769e-02  6.57534823e-02  2.26996914e-02  2.18100213e-02\n",
            " -3.00688781e-02  1.54186022e-02  6.95953667e-02 -3.88419591e-02\n",
            " -1.09261826e-01 -7.51076173e-03  1.19599029e-02  1.27546731e-02\n",
            "  1.89590286e-02  4.54232469e-02 -4.60910052e-02 -5.17162913e-03\n",
            " -1.17528150e-02 -8.67660902e-03 -2.08858494e-02  4.49374728e-02\n",
            "  1.55425863e-02  1.32864192e-02 -3.67461182e-02  1.40869562e-02\n",
            "  2.77771358e-03  2.77871802e-03  2.99189761e-02 -3.01352497e-02\n",
            " -4.63992245e-02 -5.60870692e-02 -7.94630218e-03  3.58322933e-02\n",
            " -2.37628315e-02  3.04555651e-02  4.38168878e-03 -1.49128698e-02\n",
            " -2.00193077e-02  4.84519405e-03 -1.40728580e-03 -3.53151783e-02\n",
            "  5.58815617e-03  7.45548215e-03  1.51486800e-03  4.03529257e-02\n",
            " -6.45009428e-03 -2.26505334e-03 -3.91197503e-02  1.05104251e-02\n",
            "  1.14451107e-02  2.85172090e-02  2.43227575e-02 -8.16608220e-02\n",
            " -4.06114236e-02  4.48721945e-02  5.76149265e-04  3.66367027e-02\n",
            " -5.07901683e-02  3.42644267e-02  2.49840580e-02  1.17401723e-02\n",
            "  1.71504598e-02  2.12811008e-02 -1.83073860e-02 -5.08700162e-02\n",
            " -1.79200135e-02  2.44996119e-02 -8.84228945e-03  1.70267019e-02\n",
            " -2.69817607e-03 -7.86308646e-02  5.88882118e-02  2.79413210e-03\n",
            "  1.18669569e-02 -3.29488926e-02  2.49917563e-02 -3.39025408e-02\n",
            " -7.46753961e-02  2.85454257e-03 -4.59518330e-03  1.36552588e-03\n",
            " -6.91540912e-02  3.54951341e-03 -1.40170855e-02  6.54010847e-03\n",
            " -5.49735241e-02  4.28331010e-02 -5.33593781e-02  3.18164960e-03\n",
            "  1.04328476e-01  3.42742130e-02  4.07343507e-02  1.89621374e-02\n",
            "  2.44270731e-02 -1.29662827e-02  6.00200072e-02  3.92833464e-02\n",
            "  7.58032277e-02 -1.51843159e-02 -7.98325054e-03  3.47589627e-02\n",
            " -1.86614189e-02 -6.96071684e-02 -7.13097528e-02  2.77239140e-02\n",
            " -3.20368260e-02  3.10048088e-02  1.26679859e-03 -6.69391209e-33\n",
            " -3.91474590e-02 -3.46212350e-02  2.06931774e-03  6.21102527e-02\n",
            " -4.16611917e-02 -9.90151335e-03 -1.67432949e-02  7.94499088e-03\n",
            " -1.07787456e-03  2.85014529e-02 -3.19683291e-02  1.79137557e-03\n",
            "  3.13649774e-02 -1.40697239e-02  1.93634946e-02  7.51156732e-03\n",
            "  3.52904573e-02 -1.16606001e-02 -2.80545885e-03 -1.19964723e-02\n",
            " -2.97139976e-02 -1.76579542e-02  4.52528372e-02 -1.38784072e-03\n",
            " -7.87149463e-03 -8.17421451e-03 -5.47760278e-02 -1.12036821e-02\n",
            " -6.26672432e-02 -2.15537958e-02  5.16275736e-03 -2.60673482e-02\n",
            " -1.97687615e-02 -2.41160132e-02 -3.39964777e-02  4.55974080e-02\n",
            " -5.38013736e-03 -5.15832976e-02  2.78135799e-02  3.86533737e-02\n",
            " -9.17189196e-02 -5.43299019e-02 -2.38128658e-02  8.47343728e-03\n",
            " -2.56150775e-02 -1.94259267e-02 -5.79073047e-03 -3.53576727e-02\n",
            "  3.68123539e-02 -4.75911610e-02 -3.93513218e-02  1.03633618e-03\n",
            " -3.56919691e-02  4.05900888e-02 -3.41655617e-03  2.35696789e-02\n",
            " -1.65533777e-02 -1.51566300e-03 -4.22694907e-02  1.85887124e-02\n",
            "  4.51937877e-02  5.00864722e-02 -3.62452343e-02 -3.38022523e-02\n",
            " -2.15226822e-02  7.74866575e-03  3.47940461e-03  8.42234935e-04\n",
            "  1.18840681e-02  6.97644129e-02  8.02957453e-03  1.04671009e-01\n",
            " -4.34278324e-02  1.09933473e-01  2.27688830e-02 -3.14176343e-02\n",
            " -1.14897424e-02 -3.55338980e-03  2.82176286e-02 -1.62151568e-02\n",
            "  6.32873625e-02  1.12804556e-02 -4.53989580e-02 -4.23892774e-02\n",
            " -4.77064103e-02 -4.93459255e-02 -3.72873875e-03  3.38707790e-02\n",
            " -3.09105944e-02  2.06780694e-02  3.08634862e-02  6.29141629e-02\n",
            "  1.70471668e-02 -1.72119737e-02 -3.77116725e-02  3.45212594e-02\n",
            " -4.09610346e-02  4.88858111e-03 -3.00612971e-02 -8.41366686e-03\n",
            " -4.09953296e-02 -3.98017019e-02 -5.39269708e-02  1.65643077e-02\n",
            "  5.96864708e-02  3.61520126e-02  4.98929396e-02  1.44997342e-02\n",
            " -1.09169669e-01 -1.43747739e-02 -1.36371739e-02  1.62526183e-02\n",
            " -1.17088342e-03 -3.09679564e-02 -2.90011745e-02 -6.66359765e-03\n",
            "  9.04128235e-03  4.31807004e-02 -2.07464322e-02 -5.69087006e-02\n",
            " -2.79608164e-02  4.16314155e-02 -6.23094961e-02  2.17974838e-02\n",
            "  2.10567866e-03  1.54056549e-02  3.57554071e-02  2.54537538e-02\n",
            "  3.60636972e-02 -7.28387013e-02 -5.19868638e-03 -2.23386660e-03\n",
            "  2.51205563e-07  4.48059663e-03  6.26779422e-02  2.36657616e-02\n",
            "  6.45827875e-02  1.77587606e-02  4.13445793e-02 -3.67186666e-02\n",
            "  5.56979813e-02 -4.12960388e-02  3.65489200e-02  7.52830878e-02\n",
            " -3.72791402e-02 -2.12006792e-02 -1.76452380e-02 -2.88426243e-02\n",
            "  2.56824512e-02 -4.92919646e-02 -8.79911557e-02 -2.83661131e-02\n",
            " -2.19023898e-02  3.70794460e-02  4.11574319e-02  7.84276798e-02\n",
            " -1.48524763e-02  6.14959840e-03 -4.01153006e-02 -2.02856008e-02\n",
            " -2.90952995e-02  6.01126300e-03  3.68368924e-02  7.31766457e-03\n",
            " -8.81807413e-03  4.70296619e-03  3.01264767e-02 -3.82542843e-03\n",
            " -6.81489194e-03  3.72341722e-02  8.78986120e-02 -2.90226680e-03\n",
            "  3.33461650e-02 -3.84543724e-02 -5.78214824e-02 -2.74080709e-02\n",
            "  1.45640289e-02  1.58608016e-02  1.84694491e-02  3.52275260e-02\n",
            " -5.63630275e-02  2.07085051e-02  3.22306380e-02 -2.99263764e-02\n",
            "  5.92911020e-02 -3.01268836e-03 -2.28292216e-03  2.80255154e-02\n",
            " -7.59406909e-02  4.06450173e-03  1.21565023e-02  1.28565952e-02\n",
            " -1.73893268e-03 -2.95145642e-02  3.77574638e-02  1.94634609e-02\n",
            "  4.80340868e-02  1.52996778e-02  5.04777320e-02 -8.81987624e-03\n",
            "  1.64886822e-34  4.77930829e-02 -6.48031896e-03 -3.31391720e-03\n",
            "  1.02901263e-02 -3.30803953e-02 -2.55397521e-02  3.78654152e-02\n",
            " -1.35550331e-02 -8.27926677e-03  2.65268143e-02 -2.01899372e-03]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerFeaturizer(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, dim, sentence_transformer_model):\n",
        "        self.dim = dim\n",
        "        self.sentence_transformer_model = sentence_transformer_model\n",
        "        # you can add any other params to be passed to the constructor here\n",
        "\n",
        "    #estimator. Since we don't have to learn anything in the featurizer, this is a no-op\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    #transformation: return the encoding of the document as returned by the transformer model \n",
        "    def transform(self, X, y=None):\n",
        "        X_t = []\n",
        "        \"\"\"\n",
        "        [TO BE IMPLEMENTED]\n",
        "        \n",
        "        Goal: TransformerFeaturizer's transform() method converts the raw text document\n",
        "        into a feature vector to be passed as input to the classifier.\n",
        "            \n",
        "        Given below is a dummy implementation that always maps it to a zero vector.\n",
        "        You have to implement this function so it uses computes a document embedding\n",
        "        of the input document using self.sentence_transformer_model. \n",
        "        This will be our feature representation of the document\n",
        "        \"\"\"\n",
        "        for doc in X:\n",
        "            # TODO: replace this dummy implementation\n",
        "            X_t.append(self.sentence_transformer_model.encode(doc, normalize_embeddings=True))\n",
        "            #X_t.append(np.zeros(self.dim))\n",
        "        return X_t"
      ],
      "metadata": {
        "id": "de0oQW1raJzY"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "models_v2 = {}\n",
        "for n in TRAIN_SIZE_EVALS:\n",
        "    print(\"Evaluating for training data size = {}\".format(n))\n",
        "    X_train_i = X_train[:n]\n",
        "    Y_train_i = Y_train[:n]\n",
        "\n",
        "    \"\"\"\n",
        "    [TO BE IMPLEMENTED]\n",
        "        \n",
        "    Goal: initialized below is a dummy sklearn Pipeline object with no steps.\n",
        "    You have to replace it with a pipeline object which contains at least two steps:\n",
        "    (1) mapping the input document to a feature vector (using TransformerFeaturizer)\n",
        "    (2) a classifier that predicts the class label using the feature output of first step\n",
        "\n",
        "    You can add other steps to preproces, post-process your data as you see fit. \n",
        "    You can also try any sklearn model architecture you want, but a linear classifier\n",
        "    will do just fine to start with\n",
        "\n",
        "    e.g. \n",
        "    pipeline = Pipeline([\n",
        "        ('featurizer', <your TransformerFeaturizer class instance here>),\n",
        "        ('classifier', <your sklearn classifier class instance here>)\n",
        "    ])\n",
        "    \"\"\"\n",
        "    \n",
        "    #pipeline = Pipeline()\n",
        "\n",
        "    pipeline = Pipeline([('featurizer', TransformerFeaturizer(dim=100, sentence_transformer_model = sentence_transformer_model)),\n",
        "        ('classifier', LogisticRegression(solver='saga', tol=.001))])\n",
        "\n",
        "\n",
        "    # train\n",
        "    pipeline.fit(X_train_i, Y_train_i)\n",
        "    # predict\n",
        "    Y_pred_i = pipeline.predict(X_test)\n",
        "    # record results\n",
        "    models_v2[n] = {\n",
        "        'pipeline': pipeline,\n",
        "        'test_predictions': Y_pred_i,\n",
        "        'accuracy': accuracy_score(Y_true, Y_pred_i),\n",
        "        'f1': f1_score(Y_true, Y_pred_i, average='weighted'),\n",
        "        'errors': sum([x != y for (x, y) in zip(Y_true, Y_pred_i)])\n",
        "    }\n",
        "    print(\"Accuracy on test set: {}\".format(accuracy_score(Y_true, Y_pred_i)))\n"
      ],
      "metadata": {
        "id": "n1nwMri8aeFi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12fc6b4b-09bd-43a9-e7f6-f2cfa503029e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating for training data size = 500\n",
            "Accuracy on test set: 0.72456\n",
            "Evaluating for training data size = 1000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 5: Report Results from previous two steps"
      ],
      "metadata": {
        "id": "qdQDQ8Sla2u3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Report results\n",
        "\n",
        "print(\"Word Vector Models: \")\n",
        "for train_size, result in models.items():\n",
        "    print(\"Train size: {0}  |  Accuracy: {1}  |  F1 score: {3} |  Num errors: {4}\".format(\n",
        "        train_size,\n",
        "        result['accuracy'],\n",
        "        result['f1'],\n",
        "        result['errors']\n",
        "    ))\n",
        "\n",
        "print(\"Pretrained Transformer Models: \")\n",
        "for train_size, result in models_v2.items():\n",
        "    print(\"Train size: {0}  |  Accuracy: {1}  |  F1 score: {3} |  Num errors: {4}\".format(\n",
        "        train_size,\n",
        "        result['accuracy'],\n",
        "        result['f1'],\n",
        "        result['errors']\n",
        "    ))"
      ],
      "metadata": {
        "id": "IpKaurcHa0Vh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 6: Data Augmentation\n",
        "\n",
        "In this section, we want to explore how to augment data efficiently to your existing training data. This is a very empirical exercise with a less well-defined playbook which means this section of the project is going to be open ended. Let us first understand what we mean by efficiency here, and why it matters:\n",
        "\n",
        "### Performance Gain (G):\n",
        "We will measure performance gain from data augmentation as the improvement in model accuracy (reduction in num. errors) on the Test dataset as defined above. \n",
        "\n",
        "### Budget (K):\n",
        "We will measure \"budget\" as the number of additional rows augmentated to the original training dataset.  In this project, the universe of data from which you will select to add to your training set is Datasets['augment'] (and downstream X_augment, Y_augment).\n",
        "\n",
        "This data is already labeled of course, but in most real-world scenarios the additional data is typically unlabeled. In order to augment it to your training data, you have to get it annotated which incurs some cost in time & money. This is the motivation to consider budget as a metric.\n",
        "\n",
        "### Efficiency (E = G / K): \n",
        "Efficiency = Performance Gain (Reduction in num errors in test set) / Budget (Number of additional rows augmented to the training dataset)\n",
        "\n",
        "We want to get the maximum gain in performance, while incurring minimum annotation cost."
      ],
      "metadata": {
        "id": "4v882hLIa6gR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Naively augmenting data by selecting (and incurring annotation cost) for K examples at random.\n",
        "\n",
        "# In the code snippet below, we show the gain in performance from augmenting data naively\n",
        "# at a few different budget values (K = 1000, 5000, 10000, 50000)\n",
        "\n",
        "models_aug = {}\n",
        "\n",
        "for K in [1000, 5000, 10000, 50000]:\n",
        "    X_train_aug = X_train + X_augment[:K]\n",
        "    Y_train_aug = Y_train + Y_augment[:K]\n",
        "\n",
        "    \"\"\"\n",
        "    [TO BE IMPLEMENTED]\n",
        "        \n",
        "    Paste your pipeline created from Steps 3 or 4 here.\n",
        "    (You can try both of them out to see what the performance gain is if any)\n",
        "\n",
        "    e.g. \n",
        "    pipeline = Pipeline([\n",
        "        ('featurizer', <your WordVectorFeaturizer class instance here>),\n",
        "        ('classifier', <your sklearn classifier class instance here>)\n",
        "    ])\n",
        "    \"\"\"\n",
        "    pipeline =  Pipeline([('featurizer', WordVectorFeaturizer(dim = 100, word_vector_model = word_vector_model)),\n",
        "    ('classifier', LogisticRegression(solver = 'saga',tol = 0.01))])\n",
        "\n",
        "    # train\n",
        "    pipeline.fit(X_train_aug, Y_train_aug)\n",
        "    # predict\n",
        "    Y_pred_i = pipeline.predict(X_test)\n",
        "    # record results\n",
        "    models_aug[K] = {\n",
        "        'pipeline': pipeline,\n",
        "        'test_predictions': Y_pred_i,\n",
        "        'accuracy': accuracy_score(Y_true, Y_pred_i),\n",
        "        'f1': f1_score(Y_true, Y_pred_i, average='weighted'),\n",
        "        'errors': sum([x != y for (x, y) in zip(Y_true, Y_pred_i)])\n",
        "    }\n",
        "    print(\"Accuracy on test set: {}\".format(accuracy_score(Y_true, Y_pred_i)))"
      ],
      "metadata": {
        "id": "R7cNpR4na-rv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### [Advanced] Suggested directions to explore to go beyond the naive augmentation:\n",
        "\n",
        "Can we be more intelligent with the data we choose to augment to the training dataset? \n",
        "\n",
        "**Idea 1**: Look at the test errors that the current model is making. How can this help us guide our \"data collection\" for augmentation? One possible idea is to select examples from the augmentation dataset that are similar to these errors and add them to the training data. Similarity can be approximated in many ways:\n",
        "1. [Jaccard distance between two texts](https://studymachinelearning.com/jaccard-similarity-text-similarity-metric-in-nlp/)\n",
        "2. L2 distance between mean word vectors (we already compute these features for the entire dataset using WordVectorFeaturizer)\n",
        "3. L2 distance between sentence transformer embedding (we already compute these features for the entire dataset using TransformerFeaturizer)\n",
        "  \n",
        "\n",
        "**Idea 2**: [Active Learning] Compute model's predictions on the augmentation dataset, and include those examples to the training dataset that the model finds \"hard\" ? (a proxy for this would be to look at cases where the output score distribution across all labels has nearly identical scores for top two or three labels). Some heuristics from this article might be helpful here: https://towardsdatascience.com/active-learning-in-machine-learning-525e61be16e5  \n",
        "\n",
        "**Idea 3**: Look at the test errors that the current model is making, and the distribution of these errors across labels. Select examples from the augmentation dataset that belong to these classes - adding more training data for labels that the curent model does not do well on, can improve performance (assuming label quality is good)"
      ],
      "metadata": {
        "id": "1CFYUvJ8bNTq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Examine current test errors\n",
        "test_errors = []\n",
        "Y_pred_i = models[25000]['test_predictions']\n",
        "\n",
        "for idx, label in enumerate(Y_true):\n",
        "    if label != Y_pred_i[idx]:\n",
        "        test_errors.append((X_test[idx], label,  Y_pred_i[idx]))\n",
        "\n",
        "print(\"Number of errors in the test set: {}\".format(len(test_errors)))\n",
        "print(\"Example errors: [example, true label, predicted label]\")\n",
        "for i in range(10):\n",
        "    print(test_errors[i])"
      ],
      "metadata": {
        "id": "4UVN6r0abKHI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "[TO BE IMPLEMENTED]\n",
        "\n",
        "Your additional data augmentation explorations go here\n",
        "\n",
        "For instance, the pseudocode for Idea (1) might look like the following:\n",
        "\n",
        "Augmented = {}\n",
        "For e in test_errors:\n",
        "   1. X_nn, y_nn = k nearest neighbors to (e) from X_augment, y_augment\n",
        "   2. Add each (x, y) from (X_nn, y_nn) to Augmented\n",
        "\n",
        "Add the Augmented examples to the training set\n",
        "Train the new model and record performance improvements\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "X2LCwb4ibYsV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}